{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Lab 1 - Disaster Tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Welcome to the first hands-on lab of this NLP Workshop. In this task, you will have the opportunity to apply some of the concepts you have learned during Class 1 regarding Text Processing and Feature Extraction from Text. For this, we have prepared a text classification task where you will try to distinguish between tweets that talk about real disasters, and those that do not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"../data/lab1_train.csv\")\n",
    "test_set = pd.read_csv(\"../data/lab1_test.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a disaster tweet? Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Disaster Tweet #1: {train_set[train_set['target'] == 1]['text'].values[1]}\")\n",
    "print(f\"Disaster Tweet #2: {train_set[train_set['target'] == 1]['text'].values[3]}\")\n",
    "print(f\"Disaster Tweet #3: {train_set[train_set['target'] == 1]['text'].values[9]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are a few examples of what is NOT a disaster tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Non-Disaster Tweet #1: {train_set[train_set['target'] == 0]['text'].values[1]}\")\n",
    "print(f\"Non-Disaster Tweet #2: {train_set[train_set['target'] == 0]['text'].values[3]}\")\n",
    "print(f\"Non-Disaster Tweet #3: {train_set[train_set['target'] == 0]['text'].values[9]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how can we distinguish them without knowing their label? Using NLP, of course! Let's start by applying some of the processing techniques you have larned."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complement the function \"preprocess_text\", to return a list of clean tokens, when receiving a chunk of text (sentences, tweets). Some specific preprocessing steps needed when working with tweets are already implemented."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note: As you can see, the regular expressions package is very usefull to implement customized preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Takes a text and returns the processed version of it.\n",
    "\n",
    "    Args:\n",
    "        text (str): raw text\n",
    "\n",
    "    Returns:\n",
    "        list: set of clean tokens containing the content of text\n",
    "    \"\"\"\n",
    "    # remove tweet username\n",
    "    text = re.sub(r'\\@[^\\s\\n\\r]+', '', text)\n",
    "    # remove stock market tickers like $GE\n",
    "    #text = re.sub(r'\\$\\w*', '', text)\n",
    "    # remove retweet text \"RT\"\n",
    "    text = re.sub(r'^RT[\\s]+', '', text)\n",
    "    # remove hyperlinks    \n",
    "    text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)\n",
    "    # remove hashtags (only the hash # sign)\n",
    "    processed_text = re.sub(r'#', '', text)\n",
    "\n",
    "    # Add more preprocessing steps below:\n",
    "    \n",
    "    # Remember the preprocessing techniques you learned:\n",
    "    # - Tokenization\n",
    "    # - Stopwords Removal\n",
    "    # - Stemming/Lemmatization\n",
    "\n",
    "    # You can also implement other preprocessing you may find useful\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function on a randomly sampled tweet from the train dataset (Give it a couple of tries to really see impact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "random_tweet = train_set['text'].values[randint(0, train_set.shape[0])]\n",
    "print(f\"Original Tweet: {random_tweet}\")\n",
    "print(f\"Processed Tweet: {preprocess_text(random_tweet)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have cleaned every entry in our dataset, you can proceed to extract features. Which method, from those we discussed in the class, do you think is best fit to distinguish disaster tweets from regular tweets? Can you think of any custom feature that might help in this specific context? In the following cells, you'll be able to try several ways of vectorizing the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement a bag-of-words vectorization, we will use the CountVectorizer function from sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note: Notice that this function expects the output from the preprocessing function to be a tokenized tweet. If you did not implement a tokenizer yet, you must re-think your preprocessing methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "\n",
    "def get_bow_representations(train_samples, test_samples, tokenizer):\n",
    "    \"\"\"Returns a bag-of-words based representation of both the train and test samples.\n",
    "\n",
    "    Args:\n",
    "        train_samples (list): List of training samples.\n",
    "        test_samples (list): List of test samples.\n",
    "        tokenizer (object): A preprocessing function that outputs a list of tokens.\n",
    "\n",
    "    Returns:\n",
    "        train_vectors, test_vectors: vectorized representations of the train and test sets, according to the BOW method.\n",
    "    \"\"\"\n",
    "\n",
    "    count_vectorizer = feature_extraction.text.CountVectorizer(tokenizer=tokenizer)\n",
    "\n",
    "    train_vectors = count_vectorizer.fit_transform(train_samples)\n",
    "\n",
    "    test_vectors = count_vectorizer.transform(test_samples)\n",
    "\n",
    "    return train_vectors, test_vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test this function and check the dimension of the resultant vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors, test_vectors = get_bow_representations(train_set['text'], test_set['text'], preprocess_text)\n",
    "print(train_vectors[0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considering the result of the previous cell, what is the number of unique words in the entire preprocessed train dataset?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF implementation is similar to the Bag-of-Words one. But instead, we use the TfidfVectorizer (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_representations(train_samples, test_samples, tokenizer):\n",
    "    \"\"\"Returns a tf-idf based representation of both the train and test samples.\n",
    "\n",
    "    Args:\n",
    "        train_samples (list): List of training samples.\n",
    "        test_samples (list): List of test samples.\n",
    "        tokenizer (object): A preprocessing function that outputs a list of tokens.\n",
    "\n",
    "    Returns:\n",
    "        train_vectors, test_vectors: vectorized representations of the train and test sets, according to the BOW method.\n",
    "    \"\"\"\n",
    "\n",
    "    tfidf_vectorizer = feature_extraction.text.TfidfVectorizer(tokenizer=tokenizer)\n",
    "\n",
    "    train_vectors = tfidf_vectorizer.fit_transform(train_samples)\n",
    "\n",
    "    test_vectors = tfidf_vectorizer.transform(test_samples)\n",
    "\n",
    "    return train_vectors, test_vectors\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test this function and check the dimension of the resultant vectors:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considering the result of the previous cell, what is the number of unique words in the entire preprocessed train dataset?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class was focused on preprocessing and feature extraction. However, you still need a model to perform the main task: Text Classification of Disaster Tweets. As so, we give you predictive functions for two baseline models: A Naive Bayes and a Logistic Regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def get_nb_predictions(train_samples, train_labels, test_samples):\n",
    "    \"\"\"Simple implementation of a Naive Bayes classifier.\n",
    "\n",
    "    Args:\n",
    "        train_samples (_type_): List of vectorized trained tweets.\n",
    "        train_labels (_type_): List of train labels.\n",
    "        test_samples (_type_): List of vectorized test tweets.\n",
    "\n",
    "    Returns:\n",
    "        preds: Predictions against the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    nb_model = MultinomialNB()\n",
    "\n",
    "    nb_model.fit(train_samples, train_labels)\n",
    "\n",
    "    return nb_model.predict(test_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_lr_predictions(train_samples, train_labels, test_samples):\n",
    "    \"\"\"Simple implementation of a Logistic Regression classifier.\n",
    "\n",
    "    Args:\n",
    "        train_samples (_type_): List of vectorized trained tweets.\n",
    "        train_labels (_type_): List of train labels.\n",
    "        test_samples (_type_): List of vectorized test tweets.\n",
    "\n",
    "    Returns:\n",
    "        preds: Predictions against the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    lr_model = LogisticRegression()\n",
    "\n",
    "    lr_model.fit(train_samples, train_labels)\n",
    "\n",
    "    return lr_model.predict(test_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your Pipeline!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to bring it all together! You got your preprocessing function, your feature extractors, and your models. Now, you can combine them according to the structure of the traditional NLP pipeline you learned about in this first class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start by getting your data in the correct format, i.e. a set of training tweets, a set of training labels, a set of test tweets, and a set of test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your train and test tweets and correspondent labels\n",
    "x_train, y_train, x_test, y_test = # COMPLETE THIS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Preprocess and get a numerical representation of the tweets. You should perform these two steps at once, since the vectorizers accept a preprocessing function as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize your text\n",
    "# Notice that you don't have to apply pre-processing first, because the vetorizers apply it themselves.\n",
    "# You must pass our preprocessing function to the feature extraction function, though.\n",
    "x_train_vectorized, x_test_vectorized = # COMPLETE THIS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get predictions against the test set, using one of the pre-implemented models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model and get predictions against the test samples\n",
    "preds = # COMPLETE THIS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Check the performance you achieve through the selected pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your model in terms of accuracy, f1-score\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "print(f\"Accuracy: {round(balanced_accuracy_score(y_test, preds), 5)}\")\n",
    "print(f\"F1-score: {round(f1_score(y_test, preds), 5)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you managed to reach this cell without any issues, you probably saw that you can reach a quite reasonable f1-score with this simple pipeline. Congratulations, you just built a decent text classifier!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge: Would you dare to improve your results? Try to add some steps to the preprocessing function, and maybe add the extraction of new, customized features to this pipeline, and see what values of performance you can reach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('nlp_fraud')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b16166a0de5e365fa98da2d864e1de2043058efedb5ded2f24b23a19f5d42696"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
